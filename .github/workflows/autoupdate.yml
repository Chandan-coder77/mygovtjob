name: Auto Update Jobs

on:
  schedule:
    - cron: "0 */6 * * *"   # ‚è≥ ‡§π‡§∞ 6 ‡§ò‡§Ç‡§ü‡•á ‡§™‡§∞ auto update
  workflow_dispatch:        # ‚ñ∂ Manual run (Run workflow button)

jobs:
  scrape_and_train:
    runs-on: ubuntu-latest

    steps:

    # Step-0: Pull fresh repo
    - name: Pull Repository
      uses: actions/checkout@v3

    # Step-1: Python setup
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    # Step-2: Install libs
    - name: Install Dependencies
      run: |
        pip install requests beautifulsoup4 pdfminer.six lxml

    # Step-3: Scraper (Multi-page extract)
    - name: Run Scraper
      run: python scraper_detail.py

    # Step-4: Train + Extractor + Merge + Validator
    - name: Train AI Memory (Auto-Merge + Cleaner + Extractor + Validator)
      run: python ai_trainer.py

    # Step-5: NEW Corrector Deep Engine V2
    - name: AI Corrector Smart Update (V2 Deep Crawl + Fix)
      run: python ai_corrector_v2.py

    # Step-6: Clean memory & reduce garbage
    - name: Clean & Compress Memory (Smart Garbage Removal)
      run: python clean_memory.py

    # Step-7: Auto backup before commit
    - name: Auto Backup before commit
      run: |
        mkdir -p backup
        cp ai_memory.json backup/ai_memory_$(date +'%d-%m-%Y_%H-%M-%S').json || echo "Backup skipped"

    # Step-8: Push to GitHub
    - name: Commit & Push Results
      run: |
        git config --global user.name "GitHub AI Bot"
        git config --global user.email "actions@github.com"
        git add -A
        git commit -m "Auto Update + Train + Correct V2 + Clean + Backup Complete ü§ñ" || echo "No changes"
        git push --force
